# -*- coding: utf-8 -*-
"""Few_Shot_MiniImageNet_withNormalize.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/188WuMGBw0COoCp7gZms8BSOUzVAjejM_

<h2>Load dataset from Google Colab</h2>

<h2>Unzip Dataset</h2>

<h2>Download Dataset</h2>
"""

from easyfsl.datasets import MiniImageNet
from torchvision.transforms import transforms

# Define the transformation to be applied to the images
transform = transforms.Compose([
    transforms.Resize((84, 84)),  # Resize the images to a consistent size
    transforms.ToTensor(),        # Convert images to tensors
])

# Specify the root directory where the MiniImageNet dataset is located
root_directory = "/home/amirreza/Desktop/few_shot/easy-few-shot-learning/data/mini_imagenet"

# Instantiate the MiniImageNet dataset for training
train_set = MiniImageNet(root=root_directory, split="train", training=True, transform=transform)

# Instantiate the MiniImageNet dataset for testing
test_set = MiniImageNet(root=root_directory, split="test", training=False, transform=transform)

print(len(test_set),len(train_set))

import matplotlib.pyplot as plt

image_index = 220

dataset = train_set

image, label = dataset[image_index]
print(image.shape)
plt.imshow(image.permute(1, 2, 0))
plt.title(f"Label: {label}")
plt.axis('off')
plt.show()

import torch
# Get the tuple from train_set
tuple_data = train_set[10]

# Iterate over each element in the tuple
max_value = float('inf')  # Initialize with a very small value

for element in tuple_data:
    if isinstance(element, torch.Tensor):
        element_max = torch.min(element)
        max_value = min(max_value, element_max)
    else:
        max_value = min(max_value, element)

print(max_value)

"""<h2>Start train and test</h2>"""

import os
import torch
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, random_split
from torch.utils.data import DataLoader
from torchvision import transforms
from torch import nn, optim

print(len(train_set),len(test_set))

import matplotlib.pyplot as plt

image_index = 750

dataset = train_set

image, label = dataset[image_index]
print(image.shape)
plt.imshow(image.permute(1, 2, 0))
plt.title(f"Label: {label}")
plt.axis('off')
plt.show()

"""<h2>Implement Prototypical Network</h2>"""

from torchvision.models import resnet18

class PrototypicalNetworks(nn.Module):
  def __init__(self,backbone:nn.Module):
    super(PrototypicalNetworks,self).__init__()
    self.backbone=backbone
  def forward(self,support_images:torch.Tensor,support_labels:torch.Tensor,query_images: torch.Tensor):

    z_support=self.backbone.forward(support_images)
    z_query=self.backbone.forward(query_images)

    n_way=len(torch.unique(support_labels))

    z_proto = torch.cat(
        [
            z_support[torch.nonzero(support_labels == label)].mean(0)
            for label in range(n_way)
        ]
    )

    dists = torch.cdist(z_query, z_proto)
    scores = -dists
    return scores

from torchsummary import summary

convolutional_network=resnet18(pretrained=True)
convolutional_network.fc=nn.Flatten()

# print(convolutional_network)

model=PrototypicalNetworks(convolutional_network).cuda()

# Print the summary of the model
summary(convolutional_network, input_size=(3, 224, 224))

"""<h2>Create Support and Query Images</h2>"""

#!pip install easyfsl

from easyfsl.samplers import TaskSampler
from easyfsl.utils import plot_images, sliding_average
from torch.utils.data import Dataset

print(len(test_set))

print(len(os.listdir('/content/easy-few-shot-learning/data/mini_imagenet')))

N_WAY = 5  # Number of classes in a task
N_SHOT = 5  # Number of images per class in the support set
N_QUERY = 10  # Number of images per class in the query set
N_EVALUATION_TASKS = 100


# # Assign the lambda function to get_labels
test_set.get_labels = lambda: [test_set[i][1] for i in range(len(test_set)) if i < len(test_set)]

test_sampler = TaskSampler(
    test_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS
)

test_loader = DataLoader(
    test_set,
    batch_sampler=test_sampler,
    num_workers=12,
    pin_memory=True,
    collate_fn=test_sampler.episodic_collate_fn,
)

print(len(test_sampler))
print(len(test_loader))

(
    example_support_images,
    example_support_labels,
    example_query_images,
    example_query_labels,
    example_class_ids,
) = next(iter(test_loader))

plot_images(example_support_images, "support images", images_per_row=N_SHOT)
plot_images(example_query_images, "query images", images_per_row=N_QUERY)

print(example_support_labels)
print(example_query_labels)

model.eval()
example_scores = model(
    example_support_images.cuda(),
    example_support_labels.cuda(),
    example_query_images.cuda(),
).detach()

_, example_predicted_labels = torch.max(example_scores.data, 1)

print(example_predicted_labels)
# print("Ground Truth / Predicted")
# for i in range(len(example_query_labels)):
#     print(
#         f"{test_dataset._characters[example_class_ids[example_query_labels[i]]]} / {test_dataset._characters[example_class_ids[example_predicted_labels[i]]]}"
#     )

from tqdm import tqdm

def evaluate_on_one_task(
    support_images: torch.Tensor,
    support_labels: torch.Tensor,
    query_images: torch.Tensor,
    query_labels: torch.Tensor,
) -> [int, int]:
    """
    Returns the number of correct predictions of query labels, and the total number of predictions.
    """
    return (
        torch.max(
            model(support_images.cuda(), support_labels.cuda(), query_images.cuda())
            .detach()
            .data,
            1,
        )[1]
        == query_labels.cuda()
    ).sum().item(), len(query_labels)


def evaluate(data_loader: DataLoader):
    # We'll count everything and compute the ratio at the end
    total_predictions = 0
    correct_predictions = 0

    # eval mode affects the behaviour of some layers (such as batch normalization or dropout)
    # no_grad() tells torch not to keep in memory the whole computational graph (it's more lightweight this way)
    model.eval()
    with torch.no_grad():
        for episode_index, (
            support_images,
            support_labels,
            query_images,
            query_labels,
            class_ids,
        ) in tqdm(enumerate(data_loader), total=len(data_loader)):

            correct, total = evaluate_on_one_task(
                support_images, support_labels, query_images, query_labels
            )

            total_predictions += total
            correct_predictions += correct

    print(
        f"Model tested on {len(data_loader)} tasks. Accuracy: {(100 * correct_predictions/total_predictions):.2f}%"
    )


evaluate(test_loader)

"""<h2>Train the model</h2>"""

N_TRAINING_EPISODES = 40000
N_VALIDATION_TASKS = 100

train_set.get_labels = lambda: [train_set[i][1] for i in range(len(train_set))]
train_sampler = TaskSampler(
    train_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_TRAINING_EPISODES
)
train_loader = DataLoader(
    train_set,
    batch_sampler=train_sampler,
    num_workers=12,
    pin_memory=True,
    collate_fn=train_sampler.episodic_collate_fn,
)

(
    example_support_images_train,
    example_support_labels_train,
    example_query_images_train,
    example_query_labels_train,
    example_class_ids_train,
) = next(iter(train_loader))

plot_images(example_support_images_train, "support images", images_per_row=N_SHOT)
plot_images(example_query_images_train, "query images", images_per_row=N_QUERY)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)


def fit(
    support_images: torch.Tensor,
    support_labels: torch.Tensor,
    query_images: torch.Tensor,
    query_labels: torch.Tensor,
) -> float:
    optimizer.zero_grad()
    classification_scores = model(
        support_images.cuda(), support_labels.cuda(), query_images.cuda()
    )

    loss = criterion(classification_scores, query_labels.cuda())
    loss.backward()
    optimizer.step()

    return loss.item()

#!pip install torchmetrics

from sklearn.metrics import accuracy_score

def calculate_accuracy(model, images, labels):
    model.eval()
    with torch.no_grad():
        logits = model(images)
        predicted_labels = torch.argmax(logits, dim=1).cuda().numpy()
    model.train()
    accuracy = accuracy_score(labels.cuda().numpy(), predicted_labels)
    return accuracy

# Train the model yourself with this cell
log_update_frequency = 10

all_loss = []
model.train()
with tqdm(enumerate(train_loader), total=len(train_loader)) as tqdm_train:
    for episode_index, (
        support_images,
        support_labels,
        query_images,
        query_labels,
        _,
    ) in tqdm_train:
        loss_value = fit(support_images, support_labels, query_images, query_labels)
        all_loss.append(loss_value)

        if episode_index % log_update_frequency == 0:
            tqdm_train.set_postfix(loss=sliding_average(all_loss, log_update_frequency))

# Save the model's state_dict
torch.save(model.state_dict(), 'prototypical40000.pth')

evaluate(test_loader)

evaluate(train_loader)
